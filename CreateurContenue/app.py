import streamlit as st
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import requests
import json
import time
import datetime
import os
from dotenv import load_dotenv
from openai import OpenAI
import numpy as np
import plotly.express as px
import plotly.graph_objects as go
from googleapiclient.discovery import build
from googleapiclient.errors import HttpError
from urllib.parse import urlparse, parse_qs
import base64
from io import BytesIO
import re
from functools import lru_cache
import hashlib
import logging
from datetime import timedelta
import sqlite3
from pathlib import Path
from Levenshtein import distance
import sys
import dateutil.parser

# Configuration du logging
logging.basicConfig(
    level=logging.INFO,  # Chang√© √† INFO pour r√©duire les logs verbeux
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('app_debug.log'),  # Log dans un fichier
        logging.StreamHandler()  # Log dans la console
    ]
)
logger = logging.getLogger(__name__)

# D√©sactiver les logs verbeux des biblioth√®ques externes
logging.getLogger("httpcore").setLevel(logging.WARNING)
logging.getLogger("httpx").setLevel(logging.WARNING)
logging.getLogger("openai").setLevel(logging.WARNING)
logging.getLogger("urllib3").setLevel(logging.WARNING)

# Log au d√©marrage de l'application
logger.info("=== D√âMARRAGE DE L'APPLICATION ===")
logger.info(f"Python version: {sys.version}")
logger.info(f"Working directory: {os.getcwd()}")

# Chargement des variables d'environnement
logger.info("Chargement des variables d'environnement...")
env_path = Path('.env')
if not env_path.exists():
    logger.warning("Fichier .env non trouv√©. Tentative de chargement depuis les variables d'environnement syst√®me.")
else:
    load_dotenv(dotenv_path=env_path)
    logger.info("Fichier .env charg√©")

# V√©rification des variables d'environnement requises
required_env_vars = {
    "YOUTUBE_API_KEY": "YouTube API",
    "OPENAI_API_KEY": "OpenAI API",
    "INSTAGRAM_RAPID_API_KEY": "Instagram API",
    "INSTAGRAM_RAPID_API_HOST": "Instagram API Host"
}

missing_vars = []
for var, service in required_env_vars.items():
    if not os.getenv(var):
        missing_vars.append(f"{var} ({service})")
        logger.error(f"Variable d'environnement manquante: {var} ({service})")

if missing_vars:
    logger.warning(f"Variables d'environnement manquantes: {', '.join(missing_vars)}")
    logger.warning("Certaines fonctionnalit√©s pourraient ne pas fonctionner correctement.")
else:
    logger.info("Toutes les variables d'environnement requises sont pr√©sentes")

# V√©rification sp√©cifique pour NEWS_API_KEY
NEWS_API_KEY = os.getenv("NEWS_API_KEY")
if not NEWS_API_KEY:
    logger.warning("NEWS_API_KEY manquante. L'analyse des actualit√©s sera limit√©e.")
else:
    logger.info("NEWS_API_KEY trouv√©e")

class APICache:
    """Gestionnaire de cache pour les requ√™tes API"""
    def __init__(self, cache_duration=3600):
        self.cache = {}
        self.cache_duration = cache_duration
        self.last_cleanup = time.time()

    def get(self, key):
        """R√©cup√®re une valeur du cache"""
        if key in self.cache:
            data, timestamp = self.cache[key]
            if time.time() - timestamp < self.cache_duration:
                return data
            else:
                del self.cache[key]
        return None

    def set(self, key, value):
        """Stocke une valeur dans le cache"""
        self.cache[key] = (value, time.time())
        self._cleanup_if_needed()

    def _cleanup_if_needed(self):
        """Nettoie le cache si n√©cessaire"""
        current_time = time.time()
        if current_time - self.last_cleanup > 3600:  # Nettoyage toutes les heures
            self.cache = {k: v for k, v in self.cache.items() 
                         if current_time - v[1] < self.cache_duration}
            self.last_cleanup = current_time

class APIRequestManager:
    """Gestionnaire de requ√™tes API avec retry et rate limiting"""
    def __init__(self, max_retries=3, retry_delay=1):
        self.max_retries = max_retries
        self.retry_delay = retry_delay
        self.last_request_time = {}
        self.min_request_interval = 0.1  # 100ms entre les requ√™tes

    def execute_request(self, platform, request_func, *args, **kwargs):
        """Ex√©cute une requ√™te API avec gestion des erreurs et rate limiting"""
        self._wait_for_rate_limit(platform)
        
        for attempt in range(self.max_retries):
            try:
                result = request_func(*args, **kwargs)
                self.last_request_time[platform] = time.time()
                return result
            except Exception as e:
                if attempt == self.max_retries - 1:
                    raise
                time.sleep(self.retry_delay * (attempt + 1))
                logger.warning(f"Tentative {attempt + 1} √©chou√©e pour {platform}: {str(e)}")

    def _wait_for_rate_limit(self, platform):
        """Attend si n√©cessaire pour respecter les limites de taux"""
        if platform in self.last_request_time:
            elapsed = time.time() - self.last_request_time[platform]
            if elapsed < self.min_request_interval:
                time.sleep(self.min_request_interval - elapsed)

class APIKeyManager:
    """Gestionnaire de cl√©s API avec mise en cache et gestion des erreurs"""
    
    def __init__(self):
        self.youtube_api_key = None
        self.news_api_key = None
        self.youtube_api_errors = 0
        self.news_api_errors = 0
        self.max_errors = 3  # Nombre maximum d'erreurs avant de d√©sactiver une cl√©
        self.error_reset_time = 3600  # Temps en secondes avant de r√©initialiser les erreurs (1 heure)
        self.last_error_reset = time.time()
        self.cache = {}  # Cache pour les r√©sultats d'API
        self.cache_duration = 3600  # Dur√©e de vie du cache en secondes (1 heure)
        
        # Chargement initial des cl√©s
        self._load_api_keys()
        
    def _load_api_keys(self):
        """Charge les cl√©s API depuis les variables d'environnement"""
        self.youtube_api_key = os.getenv("YOUTUBE_API_KEY")
        self.news_api_key = os.getenv("NEWS_API_KEY")
        
        if not self.youtube_api_key:
            logger.warning("Cl√© YouTube API non trouv√©e")
        if not self.news_api_key:
            logger.warning("Cl√© News API non trouv√©e")
    
    def get_youtube_api_key(self):
        """R√©cup√®re la cl√© YouTube API si elle est disponible"""
        # V√©rifier si on doit r√©initialiser les erreurs
        self._check_error_reset()
        
        if self.youtube_api_errors >= self.max_errors:
            logger.warning("Cl√© YouTube API d√©sactiv√©e en raison de trop d'erreurs")
            return None
            
        return self.youtube_api_key
    
    def get_news_api_key(self):
        """R√©cup√®re la cl√© News API si elle est disponible"""
        # V√©rifier si on doit r√©initialiser les erreurs
        self._check_error_reset()
        
        if self.news_api_errors >= self.max_errors:
            logger.warning("Cl√© News API d√©sactiv√©e en raison de trop d'erreurs")
            return None
            
        return self.news_api_key
    
    def mark_youtube_api_error(self):
        """Marque une erreur pour la cl√© YouTube API"""
        self.youtube_api_errors += 1
        logger.warning(f"Erreur YouTube API ({self.youtube_api_errors}/{self.max_errors})")
    
    def mark_news_api_error(self):
        """Marque une erreur pour la cl√© News API"""
        self.news_api_errors += 1
        logger.warning(f"Erreur News API ({self.news_api_errors}/{self.max_errors})")
    
    def _check_error_reset(self):
        """V√©rifie si on doit r√©initialiser les compteurs d'erreurs"""
        current_time = time.time()
        if current_time - self.last_error_reset > self.error_reset_time:
            if self.youtube_api_errors > 0 or self.news_api_errors > 0:
                logger.info("R√©initialisation des compteurs d'erreurs API")
            self.youtube_api_errors = 0
            self.news_api_errors = 0
            self.last_error_reset = current_time
    
    def get_cached_result(self, key):
        """R√©cup√®re un r√©sultat du cache s'il est valide"""
        if key in self.cache:
            result, timestamp = self.cache[key]
            if time.time() - timestamp < self.cache_duration:
                logger.debug(f"R√©sultat trouv√© dans le cache pour {key}")
                return result
            else:
                logger.debug(f"R√©sultat expir√© dans le cache pour {key}")
                del self.cache[key]
        return None
    
    def cache_result(self, key, result):
        """Met en cache un r√©sultat"""
        self.cache[key] = (result, time.time())
        logger.debug(f"R√©sultat mis en cache pour {key}")
    
    def clear_cache(self):
        """Vide le cache"""
        self.cache = {}
        logger.info("Cache vid√©")

class CacheManager:
    def __init__(self, cache_duration=3600):  # 1 heure par d√©faut
        self.cache = {}
        self.cache_duration = cache_duration
        self.last_cleanup = time.time()
        self.cleanup_interval = 300  # Nettoyage toutes les 5 minutes

    def get(self, key):
        self._cleanup_if_needed()
        if key in self.cache:
            data, timestamp = self.cache[key]
            if time.time() - timestamp < self.cache_duration:
                return data
            else:
                del self.cache[key]
        return None

    def set(self, key, value):
        self.cache[key] = (value, time.time())

    def _cleanup_if_needed(self):
        current_time = time.time()
        if current_time - self.last_cleanup > self.cleanup_interval:
            self._cleanup()
            self.last_cleanup = current_time

    def _cleanup(self):
        current_time = time.time()
        expired_keys = [
            key for key, (_, timestamp) in self.cache.items()
            if current_time - timestamp >= self.cache_duration
        ]
        for key in expired_keys:
            del self.cache[key]

class RetryManager:
    """Gestionnaire de tentatives avec backoff exponentiel"""
    def __init__(self, max_retries=3, base_delay=1):
        self.max_retries = max_retries
        self.base_delay = base_delay
    
    def execute_with_retry(self, func, *args, **kwargs):
        """Ex√©cute une fonction avec retry en cas d'√©chec"""
        last_error = None
        for attempt in range(self.max_retries):
            try:
                return func(*args, **kwargs)
            except Exception as e:
                last_error = e
                delay = self.base_delay * (2 ** attempt)
                logger.warning(f"Tentative {attempt + 1} √©chou√©e: {str(e)}. Nouvelle tentative dans {delay}s")
                time.sleep(delay)
        
        logger.error(f"Toutes les tentatives ont √©chou√©: {str(last_error)}")
        raise last_error

# Initialisation des gestionnaires globaux
api_manager = APIKeyManager()
cache_manager = CacheManager()
retry_manager = RetryManager()

# Configuration des API keys avec le nouveau gestionnaire
YOUTUBE_API_KEY = api_manager.get_youtube_api_key()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
INSTAGRAM_RAPID_API_KEY = os.getenv("INSTAGRAM_RAPID_API_KEY")
INSTAGRAM_RAPID_API_HOST = os.getenv("INSTAGRAM_RAPID_API_HOST")

# Initialisation du client OpenAI
client = OpenAI(api_key=OPENAI_API_KEY)

# Initialisation de l'API YouTube avec retry
def get_youtube_client():
    """Cr√©e un client YouTube avec la cl√© API actuelle"""
    return build(
        "youtube", "v3",
        developerKey=api_manager.get_youtube_api_key(),
        cache_discovery=False
    )

youtube = get_youtube_client()

class FallbackData:
    """Classe pour g√©rer les donn√©es de fallback de mani√®re centralis√©e"""
    YOUTUBE_BENCHMARKS = {
        "poor": 1.0,
        "average": 2.0,
        "good": 4.0,
        "excellent": 6.0
    }

    INSTAGRAM_BENCHMARKS = {
        "poor": 1.0,
        "average": 3.0,
        "good": 5.0,
        "excellent": 7.0
    }

    @staticmethod
    def get_industry_average():
        """Retourne la moyenne d'engagement du secteur"""
        return 3.5  # Valeur moyenne typique pour le secteur

    @staticmethod
    def get_creator_specific_stats(creator_name):
        """Retourne des statistiques sp√©cifiques pour certains cr√©ateurs"""
        creators = {
            "squeezie": {
                "short_videos": {
                    "count": 12,
                    "total_views": 36000000,
                    "total_likes": 2160000,
                    "total_comments": 180000,
                    "avg_views": 3000000,
                    "avg_likes": 180000,
                    "avg_comments": 15000,
                    "engagement_rate": 6.5
                },
                "medium_videos": {
                    "count": 15,
                    "total_views": 75000000,
                    "total_likes": 3750000,
                    "total_comments": 300000,
                    "avg_views": 5000000,
                    "avg_likes": 250000,
                    "avg_comments": 20000,
                    "engagement_rate": 5.8
                },
                "long_videos": {
                    "count": 8,
                    "total_views": 32000000,
                    "total_likes": 1920000,
                    "total_comments": 160000,
                    "avg_views": 4000000,
                    "avg_likes": 240000,
                    "avg_comments": 20000,
                    "engagement_rate": 6.2
                }
            },
            "norman": {
                "short_videos": {
                    "count": 8,
                    "total_views": 24000000,
                    "total_likes": 960000,
                    "total_comments": 48000,
                    "avg_views": 3000000,
                    "avg_likes": 120000,
                    "avg_comments": 6000,
                    "engagement_rate": 4.2
                },
                "medium_videos": {
                    "count": 12,
                    "total_views": 48000000,
                    "total_likes": 2400000,
                    "total_comments": 144000,
                    "avg_views": 4000000,
                    "avg_likes": 200000,
                    "avg_comments": 12000,
                    "engagement_rate": 5.3
                },
                "long_videos": {
                    "count": 3,
                    "total_views": 9000000,
                    "total_likes": 450000,
                    "total_comments": 36000,
                    "avg_views": 3000000,
                    "avg_likes": 150000,
                    "avg_comments": 12000,
                    "engagement_rate": 5.4
                }
            }
        }
        return creators.get(creator_name.lower())

    @staticmethod
    def get_generic_stats(platform):
        """Retourne des statistiques g√©n√©riques pour une plateforme"""
        if platform.lower() == "youtube":
            return {
                "short_videos": {
                    "count": 15,
                    "total_views": 1500000,
                    "total_likes": 75000,
                    "total_comments": 5000,
                    "avg_views": 100000,
                    "avg_likes": 5000,
                    "avg_comments": 333,
                    "engagement_rate": 5.33
                },
                "medium_videos": {
                    "count": 20,
                    "total_views": 2000000,
                    "total_likes": 100000,
                    "total_comments": 8000,
                    "avg_views": 100000,
                    "avg_likes": 5000,
                    "avg_comments": 400,
                    "engagement_rate": 5.4
                },
                "long_videos": {
                    "count": 10,
                    "total_views": 1000000,
                    "total_likes": 50000,
                    "total_comments": 4000,
                    "avg_views": 100000,
                    "avg_likes": 5000,
                    "avg_comments": 400,
                    "engagement_rate": 5.4
                }
            }
        else:  # Instagram
            return {
                "photos": {
                    "count": 30,
                    "total_likes": 150000,
                    "total_comments": 5000,
                    "avg_likes": 5000,
                    "avg_comments": 167,
                    "engagement_rate": 5.17
                },
                "videos": {
                    "count": 15,
                    "total_likes": 90000,
                    "total_comments": 3000,
                    "avg_likes": 6000,
                    "avg_comments": 200,
                    "engagement_rate": 6.2
                },
                "carousels": {
                    "count": 10,
                    "total_likes": 60000,
                    "total_comments": 2000,
                    "avg_likes": 6000,
                    "avg_comments": 200,
                    "engagement_rate": 6.2
                }
            }

class DataManager:
    """Gestionnaire centralis√© des donn√©es"""
    def __init__(self, use_api=True):
        self.cache = APICache()
        self.request_manager = APIRequestManager()
        self.fallback_data = FallbackData()
        self.use_api = use_api
        self.news_api_key = os.getenv("NEWS_API_KEY")
        
    def get_platform_data(self, username, platform):
        """R√©cup√®re les donn√©es d'une plateforme avec gestion du cache"""
        cache_key = f"{platform}_{username}_{'api' if self.use_api else 'demo'}"
        cached_data = self.cache.get(cache_key)
        
        if cached_data:
            return cached_data
            
        try:
            if platform.lower() == "youtube":
                data = self._fetch_youtube_data(username)
            else:
                data = self._fetch_instagram_data(username)
                
            if data:
                self.cache.set(cache_key, data)
                return data
                
        except Exception as e:
            logger.error(f"Erreur lors de la r√©cup√©ration des donn√©es {platform}: {str(e)}")
            if self.use_api:
                return None
            # Toujours retourner la structure compl√®te attendue
            stats = self.fallback_data.get_generic_stats(platform)
            return {
                "platform_data": {},
                "engagement_metrics": {},
                "reputation_data": {},
                "video_stats": stats if platform.lower() == "youtube" else {},
                "post_stats": stats if platform.lower() == "instagram" else {}
            }
            
    def _fetch_youtube_data(self, username):
        """R√©cup√®re les donn√©es YouTube brutes"""
        channel_id = self._get_youtube_channel_id(username)
        if not channel_id:
            return None
        
        if self.use_api:
            stats = self.request_manager.execute_request(
                "youtube",
                lambda: self._get_youtube_stats_api(channel_id)
            )
        else:
            stats = self._get_youtube_stats_demo("demo")
        
        if not stats:
            return None
        
        return {
            "platform_data": stats,
            "engagement_metrics": self._calculate_engagement(stats),
            "reputation_data": self._analyze_reputation(username),
            "video_stats": self._analyze_video_stats(channel_id)
        }
        
    def _fetch_instagram_data(self, username):
        """R√©cup√®re les donn√©es Instagram brutes"""
        if self.use_api:
            stats = self.request_manager.execute_request(
                "instagram",
                lambda: self._get_instagram_stats_api(username)
            )
        else:
            stats = self._get_instagram_stats_demo(username)
        
        if not stats:
            return None
        
        return {
            "platform_data": stats,
            "engagement_metrics": self._calculate_engagement(stats),
            "reputation_data": self._analyze_reputation(username),
            "post_stats": self._analyze_post_stats(username)
        }
        
    def _calculate_engagement(self, stats):
        """Calcule les m√©triques d'engagement"""
        try:
            views = int(stats.get("viewCount", 0))
            subscribers = int(stats.get("subscriberCount", 0))
            likes = int(stats.get("likeCount", 0))
            if self.use_api:
                # En mode API, on ne calcule le taux d'engagement que si on a des likes
                if subscribers == 0 or likes == 0:
                    return {"overall_engagement_rate": 0, "benchmark": "unknown", "industry_average": self.fallback_data.get_industry_average()}
            else:
                if subscribers == 0:
                    return {"overall_engagement_rate": 0, "benchmark": "unknown", "industry_average": self.fallback_data.get_industry_average()}
            engagement_rate = (likes / subscribers) * 100 if subscribers else 0
            benchmark = self._get_engagement_benchmark(engagement_rate)
            return {
                "overall_engagement_rate": engagement_rate,
                "benchmark": benchmark,
                "industry_average": self.fallback_data.get_industry_average()
            }
        except Exception as e:
            logger.error(f"Erreur lors du calcul de l'engagement: {str(e)}")
            return {"overall_engagement_rate": 0, "benchmark": "unknown", "industry_average": self.fallback_data.get_industry_average()}
        
    def _get_engagement_benchmark(self, rate):
        if rate >= 10:
            return "excellent"
        elif rate >= 5:
            return "good"
        elif rate >= 2:
            return "average"
        elif rate >= 1:
            return "poor"
        else:
            return "below_average"
        
    def _analyze_sentiment(self, text):
        """Analyse le sentiment d'un texte avec OpenAI"""
        try:
            response = client.chat.completions.create(
                model="gpt-4o",
                messages=[
                    {"role": "system", "content": "Tu es un expert en analyse de sentiment. R√©ponds uniquement avec un score entre -1 (tr√®s n√©gatif) et 1 (tr√®s positif)."},
                    {"role": "user", "content": f"Analyse le sentiment de ce texte et donne un score entre -1 et 1: {text}"}
                ],
                temperature=0,
                max_tokens=10
            )
            try:
                sentiment_score = float(response.choices[0].message.content.strip())
                return max(-1, min(1, sentiment_score))  # Assure que le score est entre -1 et 1
            except ValueError:
                logger.error("Impossible de convertir la r√©ponse OpenAI en score")
                return 0
        except Exception as e:
            logger.error(f"Erreur lors de l'analyse de sentiment: {str(e)}")
            return 0

    def _analyze_reputation(self, username):
        """Analyse la r√©putation du cr√©ateur (API ou d√©mo)"""
        # Mots-cl√©s √©tendus pour la d√©tection des pol√©miques
        CONTROVERSY_KEYWORDS = {
            "pol√©mique": -0.8,
            "scandale": -0.9,
            "controverse": -0.7,
            "bad buzz": -0.6,
            "accus√©": -0.7,
            "racisme": -1.0,
            "harc√®lement": -1.0,
            "agression": -1.0,
            "plainte": -0.8,
            "proc√®s": -0.7,
            "critique": -0.4,
            "clash": -0.5,
            "drama": -0.5,
            "fake": -0.6,
            "mensonge": -0.7,
            "tricherie": -0.8,
            "boycott": -0.8,
            "fraude": -0.9,
            "manipulation": -0.8,
            "malversation": -0.9
        }

        try:
            # Construire l'URL de recherche Google News
            search_query = f"{username} youtube OR {username} influenceur"
            encoded_query = requests.utils.quote(search_query)
            url = f"https://news.google.com/rss/search?q={encoded_query}&hl=fr&gl=FR&ceid=FR:fr"
            
            logger.info(f"Tentative d'appel √† Google News RSS pour {username}")
            response = requests.get(url)
            logger.info(f"R√©ponse Google News RSS re√ßue avec status code: {response.status_code}")
            
            if response.status_code == 200:
                # Parser le XML
                from xml.etree import ElementTree as ET
                root = ET.fromstring(response.content)
                
                # Extraire les articles
                articles = []
                for item in root.findall('.//item'):
                    article = {
                        "title": item.find('title').text,
                        "description": item.find('description').text,
                        "url": item.find('link').text,
                        "date": item.find('pubDate').text,
                        "source": "Google News"
                    }
                    articles.append(article)
                
                logger.info(f"Nombre d'articles trouv√©s pour {username}: {len(articles)}")
                
                # Si aucun article pertinent n'est trouv√©
                if not articles:
                    return {
                        "score": "Non disponible",
                        "risk_level": "unknown",
                        "status": "no_data",
                        "summary": f"Aucun article mentionnant {username} trouv√©.",
                        "controversies": [],
                        "all_articles": [],
                        "metrics": {
                            "average_sentiment": 0,
                            "controversy_score": 0,
                            "articles_analyzed": 0
                        }
                    }
                
                # Initialisation des variables d'analyse
                controversies = []
                all_articles = []
                total_sentiment = 0
                weighted_controversy_score = 0
                articles_analyzed = 0
                
                for article in articles:
                    title = article["title"].lower()
                    description = article["description"].lower()
                    full_text = f"{title} {description}"
                    
                    # Analyse du sentiment
                    sentiment = self._analyze_sentiment(full_text)
                    total_sentiment += sentiment
                    articles_analyzed += 1
                    
                    # D√©tection des mots-cl√©s de controverse
                    article_controversy_score = 0
                    found_keywords = []
                    
                    for keyword, impact in CONTROVERSY_KEYWORDS.items():
                        if keyword in title or keyword in description:
                            article_controversy_score += impact
                            found_keywords.append(keyword)
                    
                    # Pr√©paration des donn√©es de l'article
                    article_data = {
                        "title": article["title"],
                        "url": article["url"],
                        "date": article["date"],
                        "source": article["source"],
                        "sentiment": sentiment,
                        "controversy_score": article_controversy_score
                    }
                    
                    if found_keywords:
                        article_data["keywords"] = found_keywords
                        controversies.append(article_data)
                        weighted_controversy_score += article_controversy_score
                    
                    all_articles.append(article_data)
                
                # Calcul des scores finaux
                avg_sentiment = total_sentiment / articles_analyzed if articles_analyzed > 0 else 0
                reputation_score = int((avg_sentiment + 1) * 50)  # Convertit [-1,1] en [0,100]
                
                # Ajustement du score en fonction des controverses
                if weighted_controversy_score < 0:
                    reputation_score = max(0, reputation_score + int(weighted_controversy_score * 10))
                
                # D√©termination du niveau de risque
                if reputation_score >= 70:
                    risk_level = "low"
                elif reputation_score >= 40:
                    risk_level = "medium"
                else:
                    risk_level = "high"
                
                # Pr√©paration du r√©sum√©
                if controversies:
                    summary = f"{len(controversies)} pol√©mique(s) d√©tect√©e(s) dans la presse sur {articles_analyzed} articles analys√©s. Score de r√©putation: {reputation_score}/100"
                    logger.warning(f"Pol√©miques d√©tect√©es pour {username}: {len(controversies)}")
                else:
                    summary = f"Aucune pol√©mique d√©tect√©e sur {articles_analyzed} articles analys√©s. Score de r√©putation: {reputation_score}/100"
                
                return {
                    "score": reputation_score,
                    "risk_level": risk_level,
                    "status": "positive" if reputation_score >= 60 else "negative",
                    "summary": summary,
                    "controversies": controversies,
                    "all_articles": all_articles,
                    "metrics": {
                        "average_sentiment": avg_sentiment,
                        "controversy_score": weighted_controversy_score,
                        "articles_analyzed": articles_analyzed
                    }
                }
                
            else:
                error_msg = f"Erreur Google News RSS: {response.status_code}"
                logger.error(error_msg)
                return {
                    "score": "Non disponible",
                    "risk_level": "unknown",
                    "status": "error",
                    "summary": f"Aucune donn√©e de r√©putation disponible ({error_msg})",
                    "controversies": [],
                    "all_articles": [],
                    "metrics": {}
                }
                
        except Exception as e:
            logger.error(f"Erreur lors de l'analyse de r√©putation: {str(e)}", exc_info=True)
            return {
                "score": "Non disponible",
                "risk_level": "unknown",
                "status": "error",
                "summary": "Aucune donn√©e de r√©putation disponible (erreur lors de l'analyse)",
                "controversies": [],
                "all_articles": [],
                "metrics": {}
            }
    
    def _analyze_video_stats(self, channel_id, period_months=None):
        if self.use_api:
            try:
                youtube = get_youtube_client()
                # R√©cup√©rer les vid√©os sur la p√©riode demand√©e
                video_ids = []
                video_info = []
                nextPageToken = None
                now = datetime.datetime.utcnow()
                cutoff = None
                if period_months:
                    cutoff = now - datetime.timedelta(days=30*period_months)
                for _ in range(10):  # Jusqu'√† 250 vid√©os max (10 pages)
                    search_response = youtube.search().list(
                        channelId=channel_id,
                        part="id,snippet",
                        order="date",
                        maxResults=25,
                        type="video",
                        pageToken=nextPageToken
                    ).execute()
                    for item in search_response["items"]:
                        published_at = dateutil.parser.parse(item["snippet"]["publishedAt"]).replace(tzinfo=None)
                        if cutoff and published_at < cutoff:
                            continue
                        video_ids.append(item["id"]["videoId"])
                        video_info.append({
                            "videoId": item["id"]["videoId"],
                            "title": item["snippet"]["title"],
                            "publishedAt": published_at
                        })
                    nextPageToken = search_response.get("nextPageToken")
                    if not nextPageToken:
                        break
                if not video_ids:
                    return {}
                # R√©cup√©rer les stats des vid√©os
                stats_response = youtube.videos().list(
                    part="statistics,snippet",
                    id=",".join(video_ids)
                ).execute()
                # Cat√©gorisation enrichie par mots-cl√©s
                categories = {
                    "gaming": ["gaming", "jeu", "game", "minecraft", "fortnite", "gta", "call of duty", "fifa"],
                    "vlog": ["vlog", "daily", "journee", "routine", "voyage", "travel"],
                    "music": ["music", "musique", "clip", "cover", "chanson", "song"],
                    "challenge": ["challenge", "d√©fi", "defi"],
                    "reaction": ["reaction", "r√©action"],
                    "tuto": ["tuto", "tutorial", "astuce", "how to", "conseil"],
                    "humour": ["humour", "dr√¥le", "blague", "sketch", "com√©die", "funny"],
                    "sport": ["sport", "football", "basket", "tennis", "match", "entrainement", "workout"],
                    "autre": []
                }
                cat_stats = {cat: {"count": 0, "total_views": 0, "total_likes": 0, "total_comments": 0} for cat in categories}
                for item in stats_response["items"]:
                    title = item["snippet"]["title"].lower()
                    stats = item["statistics"]
                    found = False
                    for cat, keywords in categories.items():
                        if any(kw in title for kw in keywords):
                            cat_stats[cat]["count"] += 1
                            cat_stats[cat]["total_views"] += int(stats.get("viewCount", 0))
                            cat_stats[cat]["total_likes"] += int(stats.get("likeCount", 0))
                            cat_stats[cat]["total_comments"] += int(stats.get("commentCount", 0))
                            found = True
                            break
                    if not found:
                        cat_stats["autre"]["count"] += 1
                        cat_stats["autre"]["total_views"] += int(stats.get("viewCount", 0))
                        cat_stats["autre"]["total_likes"] += int(stats.get("likeCount", 0))
                        cat_stats["autre"]["total_comments"] += int(stats.get("commentCount", 0))
                # Calcul des moyennes et taux d'engagement par cat√©gorie
                result = {}
                for cat, vals in cat_stats.items():
                    if vals["count"] > 0:
                        avg_views = vals["total_views"] // vals["count"]
                        avg_likes = vals["total_likes"] // vals["count"]
                        avg_comments = vals["total_comments"] // vals["count"]
                        engagement_rate = (avg_likes / avg_views * 100) if avg_views else 0
                        result[cat] = {
                            "count": vals["count"],
                            "avg_views": avg_views,
                            "avg_likes": avg_likes,
                            "avg_comments": avg_comments,
                            "engagement_rate": engagement_rate
                        }
                return result
            except Exception as e:
                logger.error(f"Erreur lors de l'analyse des vid√©os API: {str(e)}")
                return {}
        else:
            # Simulation d'analyse des vid√©os
            return {
                "gaming": {
                    "count": 10,
                    "avg_views": 100000,
                    "avg_likes": 5000,
                    "engagement_rate": 5.0
                },
                "vlog": {
                    "count": 5,
                    "avg_views": 50000,
                    "avg_likes": 2500,
                    "engagement_rate": 5.0
                }
            }
    def _analyze_post_stats(self, username):
        if self.use_api:
            return {}
        else:
            return {
                "photos": {
                    "count": 20,
                    "avg_likes": 1000,
                    "avg_comments": 50,
                    "engagement_rate": 3.0
                },
                "videos": {
                    "count": 10,
                    "avg_likes": 2000,
                    "avg_comments": 100,
                    "engagement_rate": 4.0
                }
            }
    def _get_youtube_channel_id(self, username):
        if self.use_api:
            try:
                youtube = get_youtube_client()
                response = youtube.search().list(
                    part="snippet",
                    q=username,
                    type="channel",
                    maxResults=1
                ).execute()
                items = response.get("items", [])
                if items:
                    return items[0]["id"]["channelId"]
                else:
                    return None
            except Exception as e:
                logger.error(f"Erreur lors de la r√©cup√©ration de l'ID YouTube: {str(e)}")
                return None
        else:
            # Simulation
            return "UC_x5XG1OV2P6uZZ5FSM9Ttw"

    def _get_youtube_stats_api(self, channel_id):
        """R√©cup√®re les statistiques YouTube via l'API"""
        try:
            youtube = get_youtube_client()
            request = youtube.channels().list(
                part="statistics,snippet",
                id=channel_id
            )
            response = request.execute()
            
            if not response["items"]:
                return None
                
            channel_data = response["items"][0]
            return {
                "channelId": channel_id,
                "title": channel_data["snippet"]["title"],
                "description": channel_data["snippet"]["description"],
                "subscriberCount": int(channel_data["statistics"]["subscriberCount"]),
                "viewCount": int(channel_data["statistics"]["viewCount"]),
                "videoCount": int(channel_data["statistics"]["videoCount"]),
                "likeCount": 0  # YouTube API ne fournit pas directement le nombre de likes
            }
        except Exception as e:
            logger.error(f"Erreur lors de la r√©cup√©ration des stats YouTube: {str(e)}")
            return None
            
    def _get_instagram_stats_api(self, username):
        """R√©cup√®re les statistiques Instagram via l'API"""
        try:
            # Utilisation de l'API RapidAPI pour Instagram
            api_key = os.getenv("INSTAGRAM_RAPID_API_KEY")
            api_host = os.getenv("INSTAGRAM_RAPID_API_HOST")
            
            if not api_key or not api_host:
                logger.warning("Cl√©s d'API Instagram non trouv√©es")
                return None
                
            # Configuration des headers pour RapidAPI
            headers = {
                "X-RapidAPI-Key": api_key,
                "X-RapidAPI-Host": api_host
            }
            
            # R√©cup√©ration des statistiques du compte Instagram
            url = f"https://{api_host}/user/info"
            params = {"username": username}
            
            response = requests.get(url, headers=headers, params=params)
            
            if response.status_code != 200:
                logger.error(f"Erreur lors de la r√©cup√©ration des stats Instagram: {response.status_code}")
                return None
                
            data = response.json()
            
            # Extraction des statistiques pertinentes
            return {
                "username": username,
                "followerCount": int(data.get("follower_count", 0)),
                "mediaCount": int(data.get("media_count", 0)),
                "likeCount": int(data.get("like_count", 0))
            }
        except Exception as e:
            logger.error(f"Erreur lors de la r√©cup√©ration des stats Instagram: {str(e)}")
            return None
            
    def _get_instagram_stats_demo(self, username):
        """R√©cup√®re des statistiques Instagram simul√©es pour le mode d√©mo"""
        # Utilisation des donn√©es de fallback pour le mode d√©mo
        stats = self.fallback_data.get_creator_specific_stats(username)
        if not stats:
            stats = self.fallback_data.get_generic_stats("instagram")
            
        return {
            "username": username,
            "followerCount": stats.get("followerCount", 10000),
            "mediaCount": stats.get("mediaCount", 100),
            "likeCount": stats.get("likeCount", 50000)
        }

class ContentCreatorAgent:
    """Agent d'analyse des cr√©ateurs de contenu"""
    def __init__(self, use_api=True):
        self.data_manager = DataManager(use_api=use_api)
        
    def get_youtube_stats(self, username):
        """R√©cup√®re les statistiques YouTube"""
        return self.data_manager.get_platform_data(username, "youtube")
        
    def get_instagram_stats(self, username):
        """R√©cup√®re les statistiques Instagram"""
        return self.data_manager.get_platform_data(username, "instagram")

def display_platform_metrics(platform_data, platform_type):
    """Affiche les m√©triques de base de la plateforme"""
    logger.info(f"Affichage des m√©triques pour {platform_type}")
    
    if not platform_data:
        logger.warning(f"Aucune donn√©e disponible pour {platform_type}")
        st.warning(f"Aucune donn√©e disponible pour {platform_type}")
        return
        
    metrics = platform_data.get("platform_data", {})
    engagement = platform_data.get("engagement_metrics", {})
    
    logger.debug(f"M√©triques brutes : {metrics}")
    logger.debug(f"Engagement : {engagement}")
    
    col1, col2, col3 = st.columns(3)
    
    if platform_type == "youtube":
        with col1:
            subscribers = int(metrics.get('subscriberCount', 0))
            logger.info(f"Nombre d'abonn√©s YouTube : {subscribers:,}")
            st.metric("Abonn√©s", f"{subscribers:,}")
        with col2:
            views = int(metrics.get('viewCount', 0))
            logger.info(f"Nombre total de vues : {views:,}")
            st.metric("Vues totales", f"{views:,}")
        with col3:
            videos = int(metrics.get('videoCount', 0))
            logger.info(f"Nombre de vid√©os : {videos:,}")
            st.metric("Vid√©os", f"{videos:,}")
    else:
        with col1:
            followers = int(metrics.get('followerCount', 0))
            logger.info(f"Nombre d'abonn√©s Instagram : {followers:,}")
            st.metric("Abonn√©s", f"{followers:,}")
        with col2:
            posts = int(metrics.get('mediaCount', 0))
            logger.info(f"Nombre de posts : {posts:,}")
            st.metric("Posts", f"{posts:,}")
        with col3:
            likes = int(metrics.get('likeCount', 0))
            logger.info(f"Nombre total de likes : {likes:,}")
            st.metric("Likes", f"{likes:,}")
            
    engagement_rate = engagement.get('overall_engagement_rate', 0)
    if engagement_rate == 0:
        st.metric("Taux d'engagement", f"{engagement_rate:.2f}%", help="Le taux d'engagement n'est pas disponible pour cette cha√Æne. Il est calcul√© √† partir des likes et abonn√©s, mais YouTube ne fournit pas toujours cette donn√©e.")
    else:
        st.metric("Taux d'engagement", f"{engagement_rate:.2f}%")
    
def display_engagement_analysis(engagement_data):
    """Affiche l'analyse d'engagement"""
    logger.info("Affichage de l'analyse d'engagement")
    
    if not engagement_data:
        logger.warning("Donn√©es d'engagement non disponibles")
        return
        
    st.subheader("Analyse d'engagement")
    
    benchmark = engagement_data.get("benchmark", "unknown")
    logger.info(f"Benchmark d'engagement : {benchmark}")
    
    benchmark_colors = {
        "excellent": "normal",
        "good": "normal",
        "average": "normal",
        "poor": "inverse",
        "below_average": "inverse",
        "unknown": "off"
    }
    
    col1, col2 = st.columns(2)
    
    with col1:
        logger.debug(f"Affichage du benchmark avec couleur : {benchmark_colors.get(benchmark, 'off')}")
        st.metric(
            "Benchmark",
            benchmark.capitalize(),
            delta=None,
            delta_color=benchmark_colors.get(benchmark, "off"),
            help="Le benchmark est bas√© sur le taux d'engagement calcul√©. S'il est 'Unknown', cela signifie que le taux d'engagement n'a pas pu √™tre calcul√©."
        )
        
    with col2:
        industry_avg = engagement_data.get("industry_average", 0)
        logger.info(f"Moyenne du secteur : {industry_avg:.2f}%")
        st.metric(
            "Moyenne du secteur",
            f"{industry_avg:.2f}%",
            delta=None
        )
        
def display_reputation_analysis(reputation_data):
    """Affiche l'analyse de r√©putation"""
    logger.info("Affichage de l'analyse de r√©putation")
    
    if not reputation_data:
        logger.warning("Donn√©es de r√©putation non disponibles")
        return
        
    st.subheader("Analyse de r√©putation")
    
    risk_level = reputation_data.get("risk_level", "unknown")
    logger.info(f"Niveau de risque : {risk_level}")
    
    risk_colors = {
        "low": "normal",
        "medium": "inverse",
        "high": "inverse",
        "unknown": "off"
    }
    
    # M√©triques principales
    col1, col2, col3 = st.columns(3)
    
    with col1:
        score = reputation_data.get('score', 'Non disponible')
        logger.info(f"Score de r√©putation : {score}")
        st.metric(
            "Score de r√©putation",
            f"{score}/100" if isinstance(score, (int, float)) else score,
            delta=None,
            delta_color=risk_colors.get(risk_level, 'off'),
            help="Score bas√© sur l'analyse de sentiment des articles et la pr√©sence de controverses"
        )
        
    with col2:
        logger.info(f"Niveau de risque : {risk_level}")
        st.metric(
            "Niveau de risque",
            risk_level.capitalize(),
            delta=None,
            delta_color=risk_colors.get(risk_level, "off"),
            help="√âvaluation du risque bas√©e sur le score de r√©putation et les controverses d√©tect√©es"
        )
        
    with col3:
        metrics = reputation_data.get("metrics", {})
        avg_sentiment = metrics.get("average_sentiment", 0)
        logger.info(f"Sentiment moyen : {avg_sentiment:.2f}")
        st.metric(
            "Sentiment moyen",
            f"{avg_sentiment:.2f}",
            delta=None,
            delta_color="normal" if avg_sentiment >= 0 else "inverse",
            help="Moyenne du sentiment des articles (-1 tr√®s n√©gatif, +1 tr√®s positif)"
        )
    
    # R√©sum√©
    summary = reputation_data.get("summary", "")
    st.info(f"üìä {summary}")
    
    # M√©triques d√©taill√©es
    if metrics:
        with st.expander("üìà M√©triques d√©taill√©es"):
            st.markdown(f"""
            - Articles analys√©s : {metrics.get('articles_analyzed', 0)}
            - Score de controverse : {metrics.get('controversy_score', 0):.2f}
            """)
    
    # Articles controvers√©s
    controversies = reputation_data.get("controversies", [])
    if controversies:
        with st.expander(f"‚ö†Ô∏è Controverses d√©tect√©es ({len(controversies)})"):
            for controversy in controversies:
                st.markdown(f"""
                ---
                **{controversy.get('title')}**  
                üì∞ Source : {controversy.get('source', 'Inconnue')}  
                üìÖ Date : {controversy.get('date', 'Inconnue')}  
                üîç Mots-cl√©s : {', '.join(controversy.get('keywords', []))}  
                üå°Ô∏è Score de controverse : {controversy.get('controversy_score', 0):.2f}  
                üîó [Lire l'article]({controversy.get('url', '#')})
                """)
    
    # Tous les articles
    all_articles = reputation_data.get("all_articles", [])
    if all_articles:
        with st.expander(f"üì∞ Tous les articles ({len(all_articles)})"):
            # Tri des articles par sentiment
            sorted_articles = sorted(all_articles, key=lambda x: x.get('sentiment', 0), reverse=True)
            
            for article in sorted_articles:
                sentiment = article.get('sentiment', 0)
                sentiment_color = "green" if sentiment > 0.2 else "red" if sentiment < -0.2 else "gray"
                st.markdown(f"""
                ---
                **{article.get('title')}**  
                üì∞ Source : {article.get('source', 'Inconnue')}  
                üìÖ Date : {article.get('date', 'Inconnue')}  
                üé≠ Sentiment : <span style='color: {sentiment_color}'>{sentiment:.2f}</span>  
                üîó [Lire l'article]({article.get('url', '#')})
                """, unsafe_allow_html=True)

def display_content_stats(stats_data, content_type):
    """Affiche les statistiques de contenu"""
    logger.info(f"Affichage des statistiques {content_type}")
    
    if not stats_data:
        logger.warning(f"Aucune statistique disponible pour {content_type}")
        st.info("Aucune statistique disponible en mode API pour ce cr√©ateur.")
        return
        
    st.subheader(f"Statistiques {content_type}")
    
    for category, data in stats_data.items():
        logger.debug(f"Statistiques pour la cat√©gorie {category}: {data}")
        with st.expander(category.capitalize()):
            col1, col2, col3, col4 = st.columns(4)
            
            with col1:
                count = data.get("count", 0)
                logger.info(f"Nombre de {category}: {count}")
                st.metric("Nombre", count)
            with col2:
                avg_views = int(data.get("avg_views", 0))
                logger.info(f"Vues moyennes pour {category}: {avg_views:,}")
                st.metric("Vues moyennes", f"{avg_views:,}")
            with col3:
                avg_likes = int(data.get("avg_likes", 0))
                logger.info(f"Likes moyens pour {category}: {avg_likes:,}")
                st.metric("Likes moyens", f"{avg_likes:,}")
            with col4:
                engagement_rate = data.get("engagement_rate", 0)
                logger.info(f"Taux d'engagement pour {category}: {engagement_rate:.2f}%")
                st.metric("Taux d'engagement", f"{engagement_rate:.2f}%")
                
def main():
    logger.info("=== D√âMARRAGE DE L'APPLICATION ===")
    st.title("Analyseur de Cr√©ateurs de Contenu")
    
    mode = st.radio(
        "Mode d'ex√©cution",
        ["API r√©elle (donn√©es live)", "D√©mo (fausses donn√©es)"]
    )
    use_api = (mode == "API r√©elle (donn√©es live)")
    logger.info(f"Mode s√©lectionn√© : {'API' if use_api else 'D√©mo'}")
    
    platform = st.selectbox(
        "Plateforme",
        ["YouTube", "Instagram"],
        help="S√©lectionnez la plateforme √† analyser"
    )
    logger.info(f"Plateforme s√©lectionn√©e : {platform}")
    
    username = st.text_input(
        "Nom d'utilisateur",
        help=f"Entrez le nom d'utilisateur {platform}"
    )
    logger.info(f"Nom d'utilisateur saisi : {username}")
    
    # S√©lecteur de p√©riode
    period_label = st.selectbox(
        "P√©riode d'analyse des vid√©os",
        ["3 derniers mois", "6 derniers mois", "12 derniers mois", "Toutes les vid√©os"],
        help="Choisissez la p√©riode sur laquelle calculer les statistiques d√©taill√©es."
    )
    period_months = {"3 derniers mois": 3, "6 derniers mois": 6, "12 derniers mois": 12, "Toutes les vid√©os": None}[period_label]
    
    if st.button("Analyser"):
        if not username:
            logger.warning("Tentative d'analyse sans nom d'utilisateur")
            st.error("Veuillez entrer un nom d'utilisateur")
            return
            
        try:
            logger.info(f"D√©but de l'analyse pour {username} sur {platform}")
            agent = ContentCreatorAgent(use_api=use_api)
            
            if platform == "YouTube":
                data = agent.get_youtube_stats(username)
            else:
                data = agent.get_instagram_stats(username)
                
            if not data:
                logger.error(f"Impossible de r√©cup√©rer les donn√©es pour {username}")
                st.error(f"Impossible de r√©cup√©rer les donn√©es pour {username}")
                return
                
            logger.info("Affichage des r√©sultats")
            # Utiliser le taux d'engagement des vid√©os r√©centes si le global est √† 0
            engagement = data.get("engagement_metrics", {})
            # On r√©cup√®re les stats vid√©os sur la p√©riode choisie
            video_stats = agent.data_manager._analyze_video_stats(
                data["platform_data"].get("channelId", data.get("platform_data", {}).get("channelId", None)),
                period_months=period_months
            ) if platform == "YouTube" else {}
            engagement_rate = engagement.get("overall_engagement_rate", 0)
            # On prend le taux d'engagement de la cat√©gorie la plus repr√©sent√©e si le global est √† 0
            if engagement_rate == 0 and video_stats:
                best_cat = max(video_stats.items(), key=lambda x: x[1]["count"])[1]
                if best_cat.get("engagement_rate", 0) > 0:
                    engagement["overall_engagement_rate"] = best_cat["engagement_rate"]
                    engagement["benchmark"] = agent.data_manager._get_engagement_benchmark(best_cat["engagement_rate"])
            display_platform_metrics(data, platform.lower())
            display_engagement_analysis(engagement)
            display_reputation_analysis(data.get("reputation_data", {}))
            
            if platform == "YouTube":
                display_content_stats(video_stats, f"des vid√©os ({period_label})")
            else:
                display_content_stats(data.get("post_stats", {}), "des posts")
                
        except Exception as e:
            logger.error(f"Erreur lors de l'analyse : {str(e)}", exc_info=True)
            st.error(f"Une erreur est survenue : {str(e)}")
            
    logger.info("=== FIN DE L'APPLICATION ===")
            
if __name__ == "__main__":
    main()